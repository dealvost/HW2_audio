# Это будет "primary config", чтобы вы могли запускать:
#   python train.py -cn finetune_deepspeech2

defaults:
  - model: deepspeech2            # используем DeepSpeech2
  - writer: wandb                 # можно заменить на cometml, если вам нужно
  - metrics: example
  - datasets: finetune_other      # <-- тут подключаем наш датасет с train-other-500, dev-other, test-other
  - dataloader: example
  - transforms: example_only_instance
  - _self_

optimizer:
  _target_: torch.optim.AdamW
  lr: 3e-5

lr_scheduler:
  _target_: torch.optim.lr_scheduler.OneCycleLR
  max_lr: 3e-4
  pct_start: 0.1
  steps_per_epoch: ${trainer.epoch_len}
  epochs: ${trainer.n_epochs}
  anneal_strategy: cos

loss_function:
  _target_: src.loss.CTCLossWrapper

text_encoder:
  _target_: src.text_encoder.CTCTextEncoder

trainer:
  n_epochs: 30                 # например, 30 эпох (выберите нужное)
  epoch_len: 4956              # либо какое-то конкретное число. Если null, будет len(dataloader)
  device: auto
  override: False
  monitor: "min val_WER_(Argmax)"
  save_period: 5
  early_stop: ${trainer.n_epochs}
  save_dir: "saved"
  seed: 42
  device_tensors: ["spectrogram", "text_encoded", "spectrogram_length", "text_encoded_length"]

  # Здесь важно:
  resume_from: null               # не резюмируем с теми же optimizer state
  from_pretrained: "/content/template_asr/saved/DeepSpeech2_clean360_2/model_best_wer.pth"
  # from_pretrained вызывает _from_pretrained() => загрузка только весов модели (без optimizer)
